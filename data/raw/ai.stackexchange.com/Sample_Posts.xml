<?xml version="1.0" encoding="utf-8"?>
<posts>
  <row Id="1" PostTypeId="1" AcceptedAnswerId="3" CreationDate="2016-08-02T15:39:14.947" Score="10" ViewCount="682" Body="&lt;p&gt;What does &quot;backprop&quot; mean? Is the &quot;backprop&quot; term basically the same as &quot;backpropagation&quot; or does it have a different meaning?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-11-16T17:56:22.093" LastActivityDate="2021-07-08T10:45:23.250" Title="What is &quot;backprop&quot;?" Tags="&lt;neural-networks&gt;&lt;backpropagation&gt;&lt;terminology&gt;&lt;definitions&gt;" AnswerCount="5" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="2" PostTypeId="1" AcceptedAnswerId="9" CreationDate="2016-08-02T15:40:20.623" Score="14" ViewCount="960" Body="&lt;p&gt;Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-02-23T22:36:19.090" LastActivityDate="2019-02-23T22:36:37.133" Title="How does noise affect generalization?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;statistical-ai&gt;&lt;generalization&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="3" PostTypeId="2" ParentId="1" CreationDate="2016-08-02T15:40:24.820" Score="15" Body="&lt;p&gt;&quot;Backprop&quot; is the same as &quot;backpropagation&quot;: it's just a shorter way to say it. It is sometimes abbreviated as &quot;BP&quot;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T15:40:24.820" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="4" PostTypeId="1" AcceptedAnswerId="12" CreationDate="2016-08-02T15:41:22.020" Score="33" ViewCount="1221" Body="&lt;p&gt;When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-19T23:54:07.813" LastActivityDate="2021-01-19T23:54:07.813" Title="How to find the optimal number of neurons per layer?" Tags="&lt;neural-networks&gt;&lt;hyperparameter-optimization&gt;&lt;artificial-neuron&gt;&lt;hyper-parameters&gt;&lt;layers&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="11" ContentLicense="CC BY-SA 3.0" />
  <row Id="6" PostTypeId="1" AcceptedAnswerId="20" CreationDate="2016-08-02T15:43:35.460" Score="7" ViewCount="275" Body="&lt;p&gt;Given the following definition of an intelligent agent (taken from a &lt;a href=&quot;http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia article&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent? &lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="2444" LastEditDate="2019-06-15T18:25:58.513" LastActivityDate="2019-06-15T18:29:55.520" Title="Are humans intelligent according to the definition of an intelligent agent?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;intelligent-agent&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="7" PostTypeId="1" CreationDate="2016-08-02T15:45:09.070" Score="10" ViewCount="596" Body="&lt;p&gt;This &lt;a href=&quot;https://www.independent.co.uk/life-style/gadgets-and-tech/news/stephen-hawking-artificial-intelligence-could-wipe-out-humanity-when-it-gets-too-clever-humans-could-become-ants-being-stepped-a6686496.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;quote by Stephen Hawking&lt;/a&gt; has been in headlines for quite some time:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Why does he say this? To put it simply: what are the possible threats from AI (that Stephen Hawking is worried about)? If we know that AI is so dangerous, why are we still promoting it? Why is it not banned?&lt;/p&gt;&#xA;&lt;p&gt;What are the adverse consequences of the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity&quot; rel=&quot;nofollow noreferrer&quot;&gt;Technological Singularity&lt;/a&gt;?&lt;/p&gt;&#xA;" OwnerUserId="26" LastEditorUserId="2444" LastEditDate="2021-01-20T00:00:31.027" LastActivityDate="2021-01-20T00:00:31.027" Title="Why does Stephen Hawking say &quot;Artificial Intelligence will kill us all&quot;?" Tags="&lt;agi&gt;&lt;superintelligence&gt;&lt;singularity&gt;&lt;ai-safety&gt;&lt;ai-takeover&gt;" AnswerCount="6" CommentCount="1" FavoriteCount="1" ClosedDate="2016-08-04T01:36:40.283" ContentLicense="CC BY-SA 4.0" />
  <row Id="9" PostTypeId="2" ParentId="2" CreationDate="2016-08-02T15:47:02.993" Score="9" Body="&lt;p&gt;Noise in the data, to a reasonable amount, may help the network to generalize better. Sometimes, it has the opposite effect. It partly depends on the kind of noise (&quot;true&quot; vs. artificial).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;ftp://ftp.sas.com/pub/neural/FAQ3.html#A_noise&quot; rel=&quot;nofollow noreferrer&quot;&gt;AI FAQ on ANN&lt;/a&gt; gives a good overview. Excerpt:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Noise in the actual data is never a good thing, since it limits the accuracy of generalization that can be achieved no matter how extensive the training set is. On the other hand, injecting artificial noise (jitter) into the inputs during training is one of several ways to improve generalization for smooth functions when you have a small training set.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In some field, such as computer vision, it's common to increase the size of the training set by copying some samples and adding some noises or other transformation.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="2444" LastEditDate="2019-02-23T22:36:37.133" LastActivityDate="2019-02-23T22:36:37.133" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="10" PostTypeId="1" AcceptedAnswerId="32" CreationDate="2016-08-02T15:47:56.593" Score="50" ViewCount="2510" Body="&lt;p&gt;I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10135" LastEditDate="2018-10-18T10:44:33.687" LastActivityDate="2022-02-24T01:25:23.263" Title="What is fuzzy logic?" Tags="&lt;deep-neural-networks&gt;&lt;terminology&gt;&lt;fuzzy-logic&gt;" AnswerCount="6" CommentCount="0" FavoriteCount="19" ContentLicense="CC BY-SA 3.0" />
  <row Id="11" PostTypeId="2" ParentId="2" CreationDate="2016-08-02T15:48:56.970" Score="9" Body="&lt;p&gt;We typically think of machine learning models as modeling two different parts of the training data--the underlying generalizable truth (the signal), and the randomness specific to that dataset (the noise).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fitting both of those parts increases training set accuracy, but fitting the signal also increases test set accuracy (and real-world performance) while fitting the noise decreases both. So we use things like regularization and dropout and similar techniques in order to make it harder to fit the noise, and so more likely to fit the signal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just increasing the amount of noise in the training data is one such approach, but seems unlikely to be as useful. Compare random jitter to adversarial boosting, for example; the first will slowly and indirectly improve robustness whereas the latter will dramatically and directly improve it.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T15:48:56.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="12" PostTypeId="2" ParentId="4" CreationDate="2016-08-02T15:50:27.867" Score="19" Body="&lt;p&gt;There is no direct way to find the optimal number of them: people empirically try and see (e.g., using cross-validation). The most common search techniques are random, manual, and grid searches. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There exist more advanced techniques such as Gaussian processes, e.g. &lt;em&gt;&lt;a href=&quot;http://arxiv.org/abs/1609.08703&quot; rel=&quot;noreferrer&quot;&gt;Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification&lt;/a&gt;, IEEE SLT 2016&lt;/em&gt;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="4" LastEditDate="2016-09-29T00:24:06.177" LastActivityDate="2016-09-29T00:24:06.177" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="13" PostTypeId="1" AcceptedAnswerId="163" CreationDate="2016-08-02T15:52:19.413" Score="8" ViewCount="162" Body="&lt;p&gt;In particular, an embedded computer (with limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but &quot;higher than wider&quot;, with the registration split over two rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to the limited resources and need for rapid, real-time processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="14723" LastEditDate="2018-04-12T02:30:42.823" LastActivityDate="2018-04-12T02:30:42.823" Title="Can a single neural network handle recognizing two types of objects, or should it be split into two smaller networks?" Tags="&lt;neural-networks&gt;&lt;image-recognition&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="15" PostTypeId="1" CreationDate="2016-08-02T15:52:50.827" Score="38" ViewCount="3742" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_test&quot;&gt;Turing Test&lt;/a&gt; was the first test of artificial intelligence and is now a bit outdated. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test&quot;&gt;Total Turing Test&lt;/a&gt; aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot;&gt;artificial general intelligence&lt;/a&gt; (strong AI)?&lt;/p&gt;&#xA;" OwnerUserId="9" LastEditorUserId="95" LastEditDate="2016-08-04T14:10:10.990" LastActivityDate="2018-04-11T20:16:35.997" Title="Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?" Tags="&lt;turing-test&gt;&lt;agi&gt;&lt;intelligent-agent&gt;&lt;narrow-ai&gt;" AnswerCount="6" CommentCount="2" FavoriteCount="8" ContentLicense="CC BY-SA 3.0" />
  <row Id="16" PostTypeId="1" AcceptedAnswerId="142" CreationDate="2016-08-02T15:53:00.447" Score="9" ViewCount="664" Body="&lt;p&gt;What is &lt;a href=&quot;https://en.wikipedia.org/wiki/Early_stopping&quot; rel=&quot;nofollow noreferrer&quot;&gt;early stopping&lt;/a&gt; in machine learning and, in general, artificial intelligence? What are the advantages of using this method? How does it help exactly?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be interested in perspectives and links to recent research.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-10-11T22:28:23.457" LastActivityDate="2022-05-29T04:37:24.917" Title="What is &quot;early stopping&quot; in machine learning?" Tags="&lt;deep-learning&gt;&lt;definitions&gt;&lt;overfitting&gt;&lt;regularization&gt;&lt;early-stopping&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="17" PostTypeId="1" AcceptedAnswerId="45" CreationDate="2016-08-02T15:53:38.273" Score="38" ViewCount="1572" Body="&lt;p&gt;I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence? Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-09-16T16:23:39.853" LastActivityDate="2020-11-17T13:21:19.530" Title="What is the concept of the technological singularity?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;agi&gt;&lt;superintelligence&gt;&lt;singularity&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="11" ContentLicense="CC BY-SA 4.0" />
  <row Id="18" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:54:26.937" Score="3" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;To put it simply in layman terms, what are the possible threats from AI? &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Currently, there are no threat. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The threat comes if humans create a so-called ultraintelligent machine, a machine that can surpass all intellectual activities by any human. This would be the last invention man would need to do, since this machine is better in inventing machines than humans are (since that is an intellectual activity).  However, this could cause the machine to invent machines that can destruct humans, and we can't stop them because they are so much smarter than we are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is all hypothetical, no one has even a clue of what an ultraintelligent machine looks like. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If we know that AI is so dangerous why are we still promoting it? Why is it not banned?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As I said before, the existence of a ultraintelligent machine is hypothetical. Artificial Intelligence has lots of useful applications (more than this answer can contain), and if we develop it, we get even more useful applications. We just have to be careful that the machines won't overtake us. &lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-02T15:54:26.937" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="19" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:54:29.263" Score="4" Body="&lt;p&gt;Because he did not yet know how far away current AI is... Working in an media AI lab, I get this question a lot. But really... we are still a long way from this. The robots still do everything we detailledly describe them to do. Instead of seeing the robot as intelligent, I would look to the human programmer for where the creativity really happens.&lt;/p&gt;&#xA;" OwnerUserId="52" LastActivityDate="2016-08-02T15:54:29.263" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="20" PostTypeId="2" ParentId="6" CreationDate="2016-08-02T15:54:45.237" Score="2" Body="&lt;p&gt;It rather depends on how one defines several of the terms used. For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Whether the term &quot;expected&quot; is interpreted in a formal (i.e.&#xA;statistical) sense.  &lt;/li&gt;&#xA;&lt;li&gt;Whether it's assumed that humans have any kind of utilitarian&#xA;&quot;performance measure&quot;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The motivation for this description of &quot;agent&quot; arose from a desire to have a quantitative model - it's not clear that such a model is a good fit for human cognition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there are alternative definitions of agents, for example the &lt;a href=&quot;https://en.wikipedia.org/wiki/Belief%E2%80%93desire%E2%80%93intention_software_model&quot; rel=&quot;nofollow noreferrer&quot;&gt;BDI model&lt;/a&gt;, which are rather more open-ended and hence more obviously applicable to humans.&lt;/p&gt;&#xA;" OwnerUserId="42" LastEditorUserId="2444" LastEditDate="2019-06-15T18:29:55.520" LastActivityDate="2019-06-15T18:29:55.520" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="21" PostTypeId="1" CreationDate="2016-08-02T15:55:15.957" Score="5" ViewCount="100" Body="&lt;p&gt;I'm worrying that my neural network has become too complex. I don't want to end up with half of the neural network doing nothing but just take up space and resources.&lt;/p&gt;&#xA;&lt;p&gt;So, what are the techniques for detecting and preventing overfitting, to avoid such problems?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-10T01:01:09.583" LastActivityDate="2021-01-10T01:03:39.270" Title="What are the techniques for detecting and preventing overfitting?" Tags="&lt;reference-request&gt;&lt;optimization&gt;&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;generalization&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="22" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:56:10.167" Score="5" Body="&lt;p&gt;It's not just Hawking, you hear variations on this refrain from a lot of people.  And given that they're mostly very smart, well educated, well informed people (Elon Musk is another, for example), it probably shouldn't be dismissed out of hand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, the basic idea seems to be this: If we create &quot;real&quot; artificial intelligence, at some point, it will be able to improve itself, which improves it's ability to improve itself, which means it can improve it's ability to improve itself even more, and so on... a runaway cascade leading to &quot;superhuman intelligence&quot;.  That is to say, leading to something that more intelligent than we area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what happens if there is an entity on this planet which is literally more intelligent than us (humans)? Would it be a threat to us?  Well, it certainly seems reasonable to speculate that it &lt;em&gt;could&lt;/em&gt; be so.   OTOH, we have no particular reason, right now, to think that it &lt;em&gt;will&lt;/em&gt; be so. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it seems that Hawking, Musk, etc. are just coming down on the more cautious / fearful side of things.  Since we don't &lt;em&gt;know&lt;/em&gt; if a superhuman AI will be dangerous or not, and given that it could be unstoppable if it were to become malicious (remember, it's smarter than we are!), it's a reasonable thing to take under consideration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eliezer Yudkowsky has also written quite a bit on this subject, including come up with the famous &quot;AI Box&quot; experiment.  I think anybody interested in this topic should read some of his material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.yudkowsky.net/singularity/aibox/&quot; rel=&quot;noreferrer&quot;&gt;http://www.yudkowsky.net/singularity/aibox/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="33" LastActivityDate="2016-08-02T15:56:10.167" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="23" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:57:19.303" Score="4" Body="&lt;p&gt;As Andrew Ng &lt;a href=&quot;http://www.theregister.co.uk/2015/03/19/andrew_ng_baidu_ai/&quot; rel=&quot;nofollow noreferrer&quot;&gt;said&lt;/a&gt;, worrying about such threat from AI is like worrying about of overpopulation on Mars. It is science fiction. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/m6jnl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/m6jnl.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, given the rise of (much weaker) robots and other (semi-)autonomous agents, the fields of the law and ethics are increasingly incorporating them, e.g. see &lt;a href=&quot;https://en.wikipedia.org/wiki/Roboethics&quot; rel=&quot;nofollow noreferrer&quot;&gt;Roboethics&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T15:57:19.303" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="24" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:57:48.363" Score="2" Body="&lt;p&gt;He says this because it can happen. If something becomes smarter than us, why would it continue to serve us? The worst case scenario is that it takes over all manufacturing processes and consumes all matter to convert it into material capable of computation, extending outward infinitely until all matter is consumed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We know that AI is dangerous but it doesn't matter because most people don't believe in it. It goes against every comfort religion has to offer. Man is the end-all-be-all of the universe and if that fact is disputed, people will feel out of place and purposeless.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fact is most people just don't acknowledge it's possible, or that it will happen in our lifetimes, even though many reputable AI experts put the occurrence of the singularity within two decades. If people truly acknowledged that AI that was smarter than them was possible, wouldn't they be living differently? Wouldn't they be looking to do things that they enjoy, knowing that whatever it is they do that they dread will be automated? Wouldn't everyone be calling for a universal basic income?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other reason we don't ban it is because its promise is so great. One researcher could be augmented by 1,000 digital research assistants. All manual labor could be automated. For the first time, technology offers us real freedom to do whatever we please.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But even in this best case scenario where it doesn't overtake us, humans still have to adapt and alter their economic system to one where labor isn't necessary. Otherwise, those who aren't technically-trained will starve and revolt.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-08-02T16:46:21.237" LastActivityDate="2016-08-02T16:46:21.237" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="25" PostTypeId="2" ParentId="7" CreationDate="2016-08-02T15:58:13.970" Score="3" Body="&lt;p&gt;There are a number of long resources to answer this sort of question: consider Stuart Armstrong's book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B00IB4N4KU&quot; rel=&quot;nofollow&quot;&gt;Smarter Than Us&lt;/a&gt;, Nick Bostrom's book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B00LOOCGB2&quot; rel=&quot;nofollow&quot;&gt;Superintelligence&lt;/a&gt;, which grew out of this &lt;a href=&quot;http://www.nickbostrom.com/views/superintelligence.pdf&quot; rel=&quot;nofollow&quot;&gt;edge.org answer&lt;/a&gt;, &lt;a href=&quot;http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html&quot; rel=&quot;nofollow&quot;&gt;Tim Urban's explanation&lt;/a&gt;, or &lt;a href=&quot;https://aisafety.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;Michael Cohen's explanation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But here's my (somewhat shorter) answer: intelligence is all about decision-making, and we don't have any reason to believe that humans are anywhere near close to being the best possible at decision-making. Once we are able to build an AI AI researcher (that is, a computer that knows how to make computers better at thinking), the economic and military relevance of humans will rapidly disappear as any decision that could be made by a human could be made better by a computer. (Why have human generals instead of robot generals, human engineers instead of robot engineers, and so on.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This isn't necessarily a catastrophe. If the Vulcans showed up tomorrow and brought better decision-making to Earth, we could avoid a lot of misery. The hard part is making sure that what we get are Vulcans who want us around and happy, instead of something that doesn't share our values.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T15:58:13.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="26" PostTypeId="1" AcceptedAnswerId="189" CreationDate="2016-08-02T15:58:31.413" Score="24" ViewCount="1426" Body="&lt;p&gt;I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Are there examples where this is already happening to a degree today?  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2020-03-08T03:17:30.877" LastActivityDate="2021-02-01T01:03:23.217" Title="How could emotional intelligence be implemented?" Tags="&lt;emotional-intelligence&gt;&lt;turing-test&gt;&lt;affective-computing&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="7" ContentLicense="CC BY-SA 3.0" />
  <row Id="27" PostTypeId="2" ParentId="15" CreationDate="2016-08-02T16:01:59.740" Score="10" Body="&lt;p&gt;The problem of the Turing Test is that it tests the machines ability to resemble humans. Not necessarily every form of AI has to resemble humans. This makes the Turing Test less reliable. However, it is still useful since it is an actual test. It is also noteworthy that there is a prize for passing or coming closest to passing the Turing Test, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Loebner_Prize&quot;&gt;Loebner Prize&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from &lt;a href=&quot;http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition&quot;&gt;Wikipedia&lt;/a&gt;). This definition is used more often and does not depend on the ability to resemble humans. However, it is harder to test this. &lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2016-08-02T16:01:59.740" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="28" PostTypeId="1" AcceptedAnswerId="143" CreationDate="2016-08-02T16:02:44.553" Score="12" ViewCount="6854" Body="&lt;p&gt;Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence? If not, how do they differ? Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-06-20T20:36:38.073" LastActivityDate="2019-06-20T21:03:43.207" Title="Is a genetic algorithm an example of artificial intelligence?" Tags="&lt;philosophy&gt;&lt;genetic-algorithms&gt;&lt;terminology&gt;" AnswerCount="5" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="29" PostTypeId="5" CreationDate="2016-08-02T16:03:16.133" Score="0" Body="" OwnerUserId="5" LastEditorUserId="5" LastEditDate="2016-08-04T14:45:26.583" LastActivityDate="2016-08-04T14:45:26.583" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="30" PostTypeId="4" CreationDate="2016-08-02T16:03:16.133" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2016-08-02T16:03:16.133" LastActivityDate="2016-08-02T16:03:16.133" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="31" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:04:09.333" Score="9" Body="&lt;p&gt;It's analogous to analogue versus digital, or the many shades of gray in between black and white: when evaluating the truthiness of a result, in binary boolean it's either true or false (0 or 1), but when utilizing fuzzy logic, it's an estimated probability between 0 and 1 (such as 0.75 being mostly probably true). It's useful for making calculated decisions when all information needed isn't necessarily available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fuzzy_logic&quot; rel=&quot;noreferrer&quot;&gt;Wikipedia has a fantastic page for this&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="62" LastActivityDate="2016-08-02T16:04:09.333" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="32" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:04:39.867" Score="53" Body="&lt;p&gt;&lt;em&gt;As complexity rises, precise statements lose meaning and meaningful statements lose precision.&lt;/em&gt; ( Lofti Zadeh ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic deals with reasoning that is approximate rather than fixed and exact. This may make the reasoning more meaningful for a human:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/xdHPJ.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/xdHPJ.png&quot; alt=&quot;Precision and significance - comic&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the&#xA;mathematical theory of fuzzy sets, which is a generalization of the classical set theory.&#xA;By introducing the notion of &lt;em&gt;degree in the verification&lt;/em&gt; of a condition, thus enabling a&#xA;condition to be in a state other than true or false, fuzzy logic provides a very valuable&#xA;flexibility for reasoning, which makes it possible to take into account inaccuracies and&#xA;uncertainties.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One advantage of fuzzy logic in order to formalize human reasoning is that the rules&#xA;are set in natural language. For example, here are some rules of conduct that a driver&#xA;follows, assuming that he does not want to lose his driver’s licence:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/TM2UE.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/TM2UE.png&quot; alt=&quot;Fuzzy logic decision table&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively, it thus seems that the input variables like in this example are approximately&#xA;appreciated by the brain, such as the degree of verification of a condition in fuzzy&#xA;logic.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I've written a short &lt;a href=&quot;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=kz2aIc8AAAAJ&amp;amp;citation_for_view=kz2aIc8AAAAJ:eQOLeE2rZwMC&quot; rel=&quot;noreferrer&quot;&gt;introduction to fuzzy logic&lt;/a&gt; that goes into a bit more details but should be very accessible.&lt;/p&gt;&#xA;" OwnerUserId="4" LastEditorUserId="2255" LastEditDate="2020-05-26T17:15:36.130" LastActivityDate="2020-05-26T17:15:36.130" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="33" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:04:57.997" Score="3" Body="&lt;p&gt;The concept of &quot;the singularity&quot; is when machines outsmart the humans. Although Stephen Hawking opinion is that this situation is inevitable, but I think it'll be very difficult to reach that point, because every A.I. algorithm needs to be programmed by humans, therefore it would be always more limited than its creator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We would probably know when that point when humanity will lose control over Artificial Intelligence where super-smart AI would be in competition with humans and maybe creating more sophisticated intelligent beings occurred, but currently, it's more like science fiction (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Skynet_(Terminator)&quot; rel=&quot;nofollow noreferrer&quot;&gt;Terminator's Skynet&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The risk could involve killing people (like self-flying war &lt;em&gt;drones&lt;/em&gt; making their own decision), destroying countries or even the whole planet (like A.I. connected to the nuclear weapons (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/WarGames&quot; rel=&quot;nofollow noreferrer&quot;&gt;WarGames&lt;/a&gt; movie), but it doesn't prove the point that the machines would be smarter than humans.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-11T16:38:28.877" LastActivityDate="2018-04-11T16:38:28.877" CommentCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="35" PostTypeId="1" CreationDate="2016-08-02T16:05:26.390" Score="96" ViewCount="15388" Body="&lt;p&gt;These two terms seem to be related, especially in their application in computer science and software engineering.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is one a subset of another?&lt;/li&gt;&#xA;&lt;li&gt;Is one a tool used to build a system for the other?&lt;/li&gt;&#xA;&lt;li&gt;What are their differences and why are they significant?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="69" LastEditorUserId="48391" LastEditDate="2021-11-18T10:18:35.200" LastActivityDate="2021-11-18T10:18:35.200" Title="What is the difference between artificial intelligence and machine learning?" Tags="&lt;machine-learning&gt;&lt;comparison&gt;&lt;terminology&gt;&lt;ai-field&gt;" AnswerCount="9" CommentCount="2" FavoriteCount="30" ContentLicense="CC BY-SA 4.0" />
  <row Id="36" PostTypeId="1" AcceptedAnswerId="114" CreationDate="2016-08-02T16:06:25.853" Score="45" ViewCount="1806" Body="&lt;p&gt;What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2016-08-02T19:13:49.690" LastActivityDate="2021-12-24T17:45:24.580" Title="To what extent can quantum computers help to develop Artificial Intelligence?" Tags="&lt;quantum-computing&gt;" AnswerCount="5" CommentCount="1" FavoriteCount="11" ContentLicense="CC BY-SA 3.0" />
  <row Id="37" PostTypeId="1" AcceptedAnswerId="73" CreationDate="2016-08-02T16:07:29.317" Score="7" ViewCount="3441" Body="&lt;p&gt;I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?&lt;/p&gt;&#xA;" OwnerUserId="55" LastActivityDate="2016-08-03T06:37:01.983" Title="What is a Markov chain and how can it be used in creating artificial intelligence?" Tags="&lt;genetic-algorithms&gt;&lt;markov-chain&gt;&lt;probability&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="5" ContentLicense="CC BY-SA 3.0" />
  <row Id="38" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:08:06.920" Score="3" Body="&lt;p&gt;This is probably more a question of philosophy than anything. In terms of how things are commonly defined, I'll say &quot;yes, genetic algorithms are part of AI&quot;.  If you pick up a comprehensive book on artificial intelligence, there will probably be a chapter on genetic algorithms (or more broadly, evolutionary algorithms). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One area that has been extensively studied in the past is the idea of using genetic algorithms to train neural networks.  I don't know if people are still actively researching this topic or not, but it at least illustrates that GA's are part of the overall rubric of AI in one regard.&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="2444" LastEditDate="2019-06-20T21:03:43.207" LastActivityDate="2019-06-20T21:03:43.207" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="39" PostTypeId="2" ParentId="15" CreationDate="2016-08-02T16:08:09.103" Score="16" Body="&lt;p&gt;The rhetorical point of the Turing Test is that it places the 'test' for 'humanity' in &lt;em&gt;observable outcomes&lt;/em&gt;, instead of in &lt;em&gt;internal components&lt;/em&gt;. If you would behave the same in interacting with an AI as you would with a person, how could &lt;em&gt;you&lt;/em&gt; know the difference between them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But that doesn't mean it's reliable, because intelligence has many different components and there are many sorts of intellectual tasks. The Turing Test, in some respects, is about the reaction of people to behavior, which is not at all reliable--remember that many people thought &lt;a href=&quot;https://en.wikipedia.org/wiki/ELIZA&quot;&gt;ELIZA&lt;/a&gt;, a very simple chatbot, was an excellent listener and got deeply emotionally involved very quickly. It calls to mind the &lt;a href=&quot;https://www.youtube.com/watch?v=dBqhIVyfsRg&quot;&gt;Ikea commercial about throwing out a lamp&lt;/a&gt;, where the emotional attachment comes &lt;em&gt;from the human viewer&lt;/em&gt; (and the music), rather than from the lamp.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Turing tests for specific economic activities are much more practically interesting--if one can write an AI that replaces an Uber driver, for example, what that will imply is much clearer than if someone can create a conversational chatbot.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:08:09.103" CommentCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="40" PostTypeId="1" AcceptedAnswerId="44" CreationDate="2016-08-02T16:08:23.377" Score="13" ViewCount="3093" Body="&lt;p&gt;What purpose does the &quot;dropout&quot; method serve and how does it improve the overall performance of the neural network?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10" LastEditDate="2016-08-02T16:10:22.307" LastActivityDate="2020-10-01T20:16:56.117" Title="What is the &quot;dropout&quot; technique?" Tags="&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;performance&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="41" PostTypeId="1" AcceptedAnswerId="65" CreationDate="2016-08-02T16:08:34.350" Score="12" ViewCount="1015" Body="&lt;p&gt;Can an AI program have an IQ? In other words, can the IQ of an AI program be measured? Like how humans can do an IQ test.&lt;/p&gt;&#xA;" OwnerUserId="72" LastEditorUserId="2444" LastEditDate="2021-12-14T21:49:50.667" LastActivityDate="2021-12-14T21:49:50.667" Title="Can the IQ of an AI program be measured?" Tags="&lt;intelligence-testing&gt;&lt;intelligence-quotient&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="42" PostTypeId="1" AcceptedAnswerId="51" CreationDate="2016-08-02T16:09:25.427" Score="7" ViewCount="3896" Body="&lt;p&gt;Why would anybody want to use &quot;hidden layers&quot;? How do they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-10T01:10:28.160" LastActivityDate="2021-01-10T22:56:08.420" Title="What is the purpose of the hidden layers?" Tags="&lt;neural-networks&gt;&lt;deep-learning&gt;&lt;deep-neural-networks&gt;&lt;hidden-layers&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="43" PostTypeId="2" ParentId="10" CreationDate="2016-08-02T16:10:01.630" Score="22" Body="&lt;p&gt;Fuzzy logic is based on regular boolean logic. Boolean logic means you are working with truth values of either true or false (or 1 or 0 if you prefer). Fuzzy logic is the same apart from you can have truth values that are in-between true and false, which is to say, you are working with any number between 0 (inclusive) and 1 (inclusive). The fact that you can have a 'partially true and partially false' truth value is where the word &amp;quot;fuzzy&amp;quot; comes from. Natural languages often use fuzzy logic like &amp;quot;that balloon is red&amp;quot; meaning that balloon could be any colour that is similar enough to red, or &amp;quot;the shower is warm&amp;quot;. Here is a rough diagram for how &amp;quot;the temperature of the shower is warm&amp;quot; could be represented in terms of fuzzy logic (the y axis being the truth value and the x-axis being the temperature):&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/G7szY.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/G7szY.png&quot; alt=&quot;y-axis=truth value of statement about temperature, x-axis=temperature&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fuzzy logic can be applied to boolean operations such as &lt;strong&gt;and&lt;/strong&gt;, &lt;strong&gt;or&lt;/strong&gt;, and &lt;strong&gt;not&lt;/strong&gt;. Note that you can define the fuzzy logic operations in different ways. One way is with the min and max functions which return the lessermost and greatermost values of the two values inputted respectively. This would work as such:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;A and B = min(A,B)&#xA;A or B  = max(A,B)&#xA;not A   = 1-A&#xA;(where A and B are real values from 0 (inclusive) to 1 (inclusive))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;When defined like this they are called the &lt;strong&gt;Zadeh operators&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Another way would be to define &lt;strong&gt;and&lt;/strong&gt; as the first argument times the second argument, which yields different outputs for the same inputs as the Zadeh &lt;strong&gt;and&lt;/strong&gt; operator (&lt;code&gt;min(0.5,0.5)=0.5, 0.5*0.5=0.25&lt;/code&gt;). Then other operators are derived based on the &lt;strong&gt;and&lt;/strong&gt; and &lt;strong&gt;not&lt;/strong&gt; operators. This would work as such:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;A and B = A*B&#xA;not A = 1-A&#xA;A or B = not ((not A) and (not B)) = 1-((1-A)*(1-B)) = 1-(1-A)*(1-B)&#xA;(where A and B are real values from 0 (inclusive) to 1 (inclusive))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;You can then use the three &amp;quot;basic fuzzy logic operations&amp;quot; to build all other &amp;quot;fuzzy logic operations&amp;quot;, just like you can use the three &amp;quot;basic boolean operations&amp;quot; to build all other &amp;quot;boolean logic operations&amp;quot;.&lt;/p&gt;&#xA;&lt;p&gt;Note that the latter definition of the three basic operations is more in line with probability theory, so could be considered the more natural choice.&lt;/p&gt;&#xA;&lt;p&gt;Sources:&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fuzzy_logic&quot; rel=&quot;nofollow noreferrer&quot;&gt;Fuzzy logic wikipedia&lt;/a&gt;,&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_algebra&quot; rel=&quot;nofollow noreferrer&quot;&gt;Boolean algebra wikipedia&lt;/a&gt;,&#xA;&lt;a href=&quot;https://www.youtube.com/watch?v=r804UF8Ia4c&quot; rel=&quot;nofollow noreferrer&quot;&gt;Explanation of fuzzy logic on Youtube&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Note: if anyone could suggest some more reliable sources in the comments, I will happily add them to the list (I understand that the current ones aren't too reliable).&lt;/p&gt;&#xA;" OwnerUserId="47" LastEditorUserId="47" LastEditDate="2022-02-24T01:25:23.263" LastActivityDate="2022-02-24T01:25:23.263" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="44" PostTypeId="2" ParentId="40" CreationDate="2016-08-02T16:12:08.767" Score="8" Body="&lt;p&gt;Dropout means that every individual data point is only used to fit a random subset of the neurons. This is done to make the neural network more like an ensemble model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, just as a random forest is averaging together the results of many individual decision trees, you can see a neural network trained using dropout as averaging together the results of many individual neural networks (with 'results' understood to mean activations at every layer, rather than just the output layer).&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="12133" LastEditDate="2018-01-17T00:30:27.303" LastActivityDate="2018-01-17T00:30:27.303" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="45" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:12:49.850" Score="25" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity&quot; rel=&quot;nofollow noreferrer&quot;&gt;technological singularity&lt;/a&gt; is a theoretical point in time at which a &lt;em&gt;self-improving&lt;/em&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;artificial general intelligence&lt;/a&gt; becomes able to understand and manipulate concepts outside of the human brain's range, that is, the moment when it can understand things humans, by biological design, can't.&lt;/p&gt;&#xA;&lt;p&gt;The fuzziness about the singularity comes from the fact that, from the singularity onwards, history is effectively &lt;em&gt;unpredictable&lt;/em&gt;. Humankind would be unable to predict any future events, or explain any present events, as science itself becomes incapable of describing machine-triggered events. Essentially, machines would think of us the same way we think of ants. Thus, we can make no predictions past the singularity. Furthermore, as a logical consequence, we'd be unable to define the point at which the singularity may occur at all, or even recognize it when it happens.&lt;/p&gt;&#xA;&lt;p&gt;However, in order for the singularity to take place, AGI needs to be developed, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence#Feasibility&quot; rel=&quot;nofollow noreferrer&quot;&gt;whether that is possible is quite a hot debate&lt;/a&gt; right now. Moreover, an algorithm that creates &lt;em&gt;superhuman intelligence&lt;/em&gt; (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Superintelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;superintelligence&lt;/em&gt;&lt;/a&gt;) out of bits and bytes would have to be designed. By definition, a human programmer wouldn't be able to do such a thing, as his/her brain would need to be able to comprehend concepts beyond its range. There is also the argument that an &lt;a href=&quot;https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;intelligence explosion&lt;/em&gt;&lt;/a&gt; (the mechanism by which a technological singularity would theoretically be formed) would be impossible due to the difficulty of the design challenge of making itself more intelligent, getting larger proportionally to its intelligence, and that the difficulty of the design itself may overtake the intelligence required to solve the said challenge.&lt;/p&gt;&#xA;&lt;p&gt;Also, there are related theories involving &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_takeover&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;machines taking over humankind&lt;/em&gt;&lt;/a&gt; and all of that sci-fi narrative. However, that's unlikely to happen, if &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_Laws_of_Robotics&quot; rel=&quot;nofollow noreferrer&quot;&gt;Asimov's laws&lt;/a&gt; are followed appropriately. &lt;a href=&quot;https://www.youtube.com/watch?app=desktop&amp;amp;v=7PKx3kS7f4A&quot; rel=&quot;nofollow noreferrer&quot;&gt;Even if Asimov's laws were not enough&lt;/a&gt;, a series of constraints would still be necessary in order to avoid the misuse of AGI by misintentioned individuals, and Asimov's laws are the nearest we have to that.&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2020-11-17T13:21:19.530" LastActivityDate="2020-11-17T13:21:19.530" CommentCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="46" PostTypeId="1" AcceptedAnswerId="71" CreationDate="2016-08-02T16:14:26.350" Score="11" ViewCount="3932" Body="&lt;p&gt;When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-01-31T16:41:08.950" LastActivityDate="2021-01-31T16:46:15.343" Title="When did Artificial Intelligence research first start?" Tags="&lt;research&gt;&lt;history&gt;&lt;ai-field&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" ContentLicense="CC BY-SA 3.0" />
  <row Id="47" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:14:29.337" Score="2" Body="&lt;p&gt;The notion of genetics used in Genetic Algorithms (GAs) is a &lt;em&gt;very&lt;/em&gt; stripped down version relative to genetics in nature, essentially consisting of a population of 'genes' (representing solutions to some predefined problem) subject to `survival of the fittest' during iterated application of recombination and mutation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, the term 'Computational Intelligence' (CI) tends to be used to describe computational techniques intended to produce `the appearance of intelligence by &lt;em&gt;any&lt;/em&gt; computational means', rather than specifically attempting to mimic the mechanisms that are believed to give rise to human (or animal) intelligence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, the distinction between CI and AI is not so hard and fast, and arguably arose during the `AI Winter' when the term AI was out of fashion.&lt;/p&gt;&#xA;" OwnerUserId="42" LastActivityDate="2016-08-02T16:14:29.337" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="48" PostTypeId="2" ParentId="42" CreationDate="2016-08-02T16:15:49.970" Score="3" Body="&lt;p&gt;Hidden layers by themselves aren't useful. If you had hidden layers that were linear, the end result would still be a linear function of the inputs, and so you could collapse an arbitrary number of linear layers down to a single layer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why we use nonlinear &lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot; rel=&quot;nofollow&quot;&gt;activation functions&lt;/a&gt;, like RELU. This allows us to add a level of nonlinear complexity with each hidden layer, and with arbitrarily many hidden layers we can construct arbitrarily complicated nonlinear functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because we can (at least in theory) capture any degree of complexity, we think of neural networks as &quot;universal learners,&quot; in that a large enough network could mimic any function.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:15:49.970" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="49" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:16:25.863" Score="24" Body="&lt;p&gt;Machine learning is a subset of artificial intelligence. Roughly speaking, it corresponds to its learning side. There is no &quot;official&quot; definitions, boundaries are a bit fuzzy.&lt;/p&gt;&#xA;" OwnerUserId="4" LastActivityDate="2016-08-02T16:16:25.863" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="50" PostTypeId="1" AcceptedAnswerId="138" CreationDate="2016-08-02T16:16:46.797" Score="4" ViewCount="5881" Body="&lt;p&gt;How would you estimate the generalization error? What are the methods of achieving this?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2020-11-07T17:26:05.080" LastActivityDate="2020-11-08T13:07:02.920" Title="How can the generalization error be estimated?" Tags="&lt;machine-learning&gt;&lt;deep-neural-networks&gt;&lt;overfitting&gt;&lt;computational-learning-theory&gt;&lt;generalization&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="51" PostTypeId="2" ParentId="42" CreationDate="2016-08-02T16:16:59.330" Score="5" Body="&lt;p&gt;&quot;Hidden&quot; layers really aren't all that special... a hidden layer is really no more than any layer that isn't input or output. So even a very simple 3 layer NN has 1 hidden layer. So I think the question isn't really &quot;How do hidden layers help?&quot; as much as &quot;Why are deeper networks better?&quot;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the answer to that latter question is an area of active research. Even top experts like Geoffrey Hinton and Andrew Ng will freely admit that we don't really understand why deep neural networks work. That is, we don't understand them in complete detail anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, the theory, as I understand it goes something like this...  successive layers of the network learn successively more sophisticated features, which build on the features from preceding layers. So, for example, an NN used for facial recognition might work like this: the first layer detects edges and nothing else. The next layer up recognizes geometric shapes (boxes, circles, etc.). The next layer up recognizes primitive features of a face, like eyes, noses, jaw, etc. The next layer up then recognizes composites based on combinations of &quot;eye&quot; features, &quot;nose&quot; features, and so on.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in theory, deeper networks (more hidden layers) are better in that they develop a more granular/detailed representation of a &quot;thing&quot; being recognized.  &lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-12T02:30:32.837" LastActivityDate="2018-04-12T02:30:32.837" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="52" PostTypeId="1" AcceptedAnswerId="1437" CreationDate="2016-08-02T16:19:30.337" Score="7" ViewCount="643" Body="&lt;p&gt;I've implemented &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot; rel=&quot;nofollow noreferrer&quot;&gt;the reinforcement learning algorithm&lt;/a&gt; for an agent to play &lt;a href=&quot;https://github.com/admonkey/snappybird&quot; rel=&quot;nofollow noreferrer&quot;&gt;snappy bird&lt;/a&gt; (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously, storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the NN on an existing q-table would work, but I would like to not use a q-table at all if possible.&lt;/p&gt;&#xA;" OwnerUserId="62" LastEditorUserId="2444" LastEditDate="2019-05-10T14:42:23.103" LastActivityDate="2019-05-10T14:42:23.103" Title="Is it possible to implement reinforcement learning using a neural network?" Tags="&lt;neural-networks&gt;&lt;reinforcement-learning&gt;&lt;deep-rl&gt;&lt;function-approximation&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="53" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:20:03.773" Score="62" Body="&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; has been defined by many people in multiple (often similar) ways [&lt;a href=&quot;http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://www.cs.ubbcluj.ro/%7Egabis/ml/ML-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;2&lt;/a&gt;]. One definition says that machine learning (ML) is the field of study that gives computers the &lt;em&gt;ability to learn&lt;/em&gt; without being explicitly programmed.&lt;/p&gt;&#xA;&lt;p&gt;Given the above definition, we might say that machine learning is geared towards problems for which we have (lots of) data (experience), from which a program can learn and can get better at a task.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Artificial intelligence&lt;/strong&gt; has many more aspects, where machines may not get better at tasks by learning from data, but may exhibit &lt;em&gt;intelligence&lt;/em&gt; through rules (e.g. expert systems like &lt;a href=&quot;https://en.wikipedia.org/wiki/Mycin&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mycin&lt;/a&gt;), &lt;a href=&quot;https://silp.iiita.ac.in/wp-content/uploads/PROLOG.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;logic&lt;/a&gt; or algorithms, e.g. path-finding).&lt;/p&gt;&#xA;&lt;p&gt;The book &lt;a href=&quot;http://aima.cs.berkeley.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;Artificial Intelligence: A Modern Approach&lt;/em&gt;&lt;/a&gt; shows more research fields of AI, like &lt;em&gt;Constraint Satisfaction Problems&lt;/em&gt;, &lt;em&gt;Probabilistic Reasoning&lt;/em&gt; or &lt;em&gt;Philosophical Foundations&lt;/em&gt;.&lt;/p&gt;&#xA;" OwnerUserId="28" LastEditorUserId="2444" LastEditDate="2021-01-17T20:24:34.557" LastActivityDate="2021-01-17T20:24:34.557" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="54" PostTypeId="1" AcceptedAnswerId="76" CreationDate="2016-08-02T16:20:40.520" Score="7" ViewCount="192" Body="&lt;p&gt;I read that in the spring of 2016 a computer &lt;a href=&quot;https://en.wikipedia.org/wiki/Computer_Go&quot; rel=&quot;nofollow noreferrer&quot;&gt;Go program&lt;/a&gt; was finally able to beat a professional human for the first time.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are some of the methods used to program the successful Go-playing program? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are those methods considered to be artificial intelligence?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-01-10T01:13:07.280" LastActivityDate="2021-01-10T01:13:07.280" Title="Does the recent advent of a Go playing computer represent Artificial Intelligence?" Tags="&lt;philosophy&gt;&lt;definitions&gt;&lt;agi&gt;&lt;alphago&gt;&lt;narrow-ai&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="56" PostTypeId="2" ParentId="35" CreationDate="2016-08-02T16:20:52.200" Score="9" Body="&lt;p&gt;Many terms have 'mostly' the same meanings, and so the differences are just in emphasis, perspective, or historical descent. People disagree as to which label refers to the superset or the subset; there are people who will call AI a branch of ML and people who will call ML a branch of AI.&lt;/p&gt;&#xA;&lt;p&gt;I typically hear Machine Learning used as a form of 'applied statistics' where we specify a learning problem in enough detail that we can just feed training data into it and get a useful model out the other side.&lt;/p&gt;&#xA;&lt;p&gt;I typically hear Artificial Intelligence as a catch-all term to refer to any sort of intelligence embedded in the environment or in code. This is a very expansive definition, and others use narrower ones (such as focusing on artificial &lt;em&gt;general&lt;/em&gt; intelligence, which is not domain-specific). (Taken to an extreme, my version includes thermostats.)&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="2444" LastEditDate="2021-01-15T20:47:05.733" LastActivityDate="2021-01-15T20:47:05.733" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="57" PostTypeId="2" ParentId="17" CreationDate="2016-08-02T16:23:13.273" Score="1" Body="&lt;p&gt;The &quot;singularity,&quot; viewed narrowly, refers to a point at which economic growth is so fast that we can't make useful predictions about what the future past that point will look like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's often used interchangeably with &quot;intelligence explosion,&quot; which is when we get so-called Strong AI, which is AI that is intelligent enough to understand and improve itself. It seems reasonable to expect that the intelligence explosion would immediately lead to an economic singularity, but the reverse is not necessarily true.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:23:13.273" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="58" PostTypeId="1" CreationDate="2016-08-02T16:25:20.223" Score="13" ViewCount="3611" Body="&lt;p&gt;Who first coined the term Artificial Intelligence? Is there a published research paper that first used that term?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-10-05T02:07:00.143" LastActivityDate="2020-04-21T11:12:25.793" Title="Who first coined the term Artificial Intelligence?" Tags="&lt;terminology&gt;&lt;history&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="60" PostTypeId="1" AcceptedAnswerId="1464" CreationDate="2016-08-02T16:27:49.533" Score="11" ViewCount="513" Body="&lt;p&gt;I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just &lt;em&gt;how&lt;/em&gt; complicated AI is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are other issues currently facing AI development?&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2018-04-12T02:34:45.870" Title="What are the main problems hindering current AI development?" Tags="&lt;machine-learning&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="61" PostTypeId="2" ParentId="36" CreationDate="2016-08-02T16:28:29.363" Score="4" Body="&lt;p&gt;Quantum computers can help further develop A.I. algorithms and solve the problems to the extent of our creativity and ability to define the problem. For example breaking cryptography can take seconds, where it can takes thousands of years for standard computers. The same with artificial intelligence, it can predict all the combinations for the given problem defined by algorithm. This is due to superposition of multiple states of quantum bits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, quantum computers are still in the early stages of development and can perform complex calculation. There are already technologies like &lt;a href=&quot;https://en.wikipedia.org/wiki/D-Wave_Systems&quot; rel=&quot;nofollow&quot;&gt;D-Wave&lt;/a&gt; systems which are used by Google and NASA for complex data analysis, using Multi-Qubit type quantum computers for &lt;a href=&quot;https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations&quot; rel=&quot;nofollow&quot;&gt;solving NSE fluid dynamics problems&lt;/a&gt; of interest or global surveillance for military purposes, and many more which we're not aware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently there are only a few quantum computers available to the public, like &lt;a href=&quot;http://www.research.ibm.com/quantum/&quot; rel=&quot;nofollow&quot;&gt;IBM Quantum Experience&lt;/a&gt; (the world’s first quantum computing platform delivered via the IBM Cloud), but it's programming on quantum logic gates levels, so we're many years behind creating artificial intelligence available to public. There are some &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_programming&quot; rel=&quot;nofollow&quot;&gt;quantum computing languages&lt;/a&gt; such as QCL, Q or Quipper, but I'm not aware any libraries which can provide artificial intelligence frameworks. It doesn't mean it's not there, and I'm sure huge companies and governments organisations are using it for their agenda to outcome the competition (like financial market analysis, etc.).&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2016-08-04T21:06:48.983" LastActivityDate="2016-08-04T21:06:48.983" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="62" PostTypeId="2" ParentId="28" CreationDate="2016-08-02T16:29:11.850" Score="2" Body="&lt;p&gt;Human intelligence is &lt;strong&gt;not&lt;/strong&gt; an example of natural genetic algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Genetic algorithms have collections of solutions that are collided with each other to make new solutions, eventually returning the best solution. Human intelligence is a network of neurons doing information processing, and almost all of it doesn't behave the same way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But that something doesn't behave in the same way that human intelligence does doesn't mean that it's not an AI algorithm; I would include 'genetic algorithms' as a numerical optimization technique, and since optimization and intelligence are deeply linked any numerical optimization technique could be seen as an AI technique.&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:29:11.850" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="63" PostTypeId="1" AcceptedAnswerId="208" CreationDate="2016-08-02T16:29:24.803" Score="11" ViewCount="237" Body="&lt;p&gt;I've read that the most of the problems can be solved with 1-2 hidden layers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you know you need more than 2? For what kind of problems you would need them (give me an example)?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="14723" LastEditDate="2018-04-11T18:49:51.953" LastActivityDate="2018-04-11T18:49:51.953" Title="What kind of problems require more than 2 hidden layers?" Tags="&lt;deep-neural-networks&gt;&lt;hidden-layers&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="64" PostTypeId="1" CreationDate="2016-08-02T16:29:29.207" Score="4" ViewCount="76" Body="&lt;p&gt;What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Beating a human at the game of chess&lt;/li&gt;&#xA;&lt;li&gt;Convincing a human that a person was conversing with them (passing the Turing test)&lt;/li&gt;&#xA;&lt;li&gt;Beating a human at Jeopardy game show&lt;/li&gt;&#xA;&lt;li&gt;Beating a human at the game of go.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Were there milestones that were considered major in the field before the 1990s?&lt;/p&gt;&#xA;" OwnerUserId="55" LastActivityDate="2017-03-15T05:38:48.220" Title="What were the first areas of research and what were some early successes?" Tags="&lt;history&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="65" PostTypeId="2" ParentId="41" CreationDate="2016-08-02T16:30:28.737" Score="11" Body="&lt;p&gt;Short answer: No.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Longer answer: It depends on what IQ exactly is, and when the question is asked compared to ongoing development. The topic you're referring to is actually more commonly described as AGI, or Artificial General Intelligence, as opposed to AI, which could be any narrow problem solving capability represented in software/hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Intelligence_quotient&quot;&gt;Intelligence quotient&lt;/a&gt; is a rough estimate of how well humans are able to generally answer questions they have not previously encountered, but as a predictor it is somewhat flawed, and has many criticisms and detractors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently (2016), no known programs have the ability to generalize, or apply learning from one domain to solving problems in an arbitrarily different domain through an abstract understanding. (However there are programs which can effectively analyze, or break down some information domains into simpler representations.) This seems likely to change as time goes on and both hardware and software techniques are developed toward this goal. Experts widely disagree as to the likely timing and approach of these developments, as well as to the most probable outcomes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth noting that there seems to be a large deficit of understanding as to what exactly consciousness is, and disagreement over whether there is ever likely to be anything in the field of artificial intelligence that compares to it.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="46" LastEditDate="2016-08-02T17:08:34.127" LastActivityDate="2016-08-02T17:08:34.127" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="66" PostTypeId="2" ParentId="58" CreationDate="2016-08-02T16:31:24.350" Score="13" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist))&quot; rel=&quot;noreferrer&quot;&gt;John McCarthy&lt;/a&gt; (1927 - 2011) was an American computer scientist. A pioneer in the foundations of artificial intelligence research, &lt;strong&gt;he coined the term &quot;artificial intelligence&quot;&lt;/strong&gt;. He was one of the creators of the (original) Lisp programming language, which was quite involved in early AI research in the 1960s and 1970s.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;He coined the term in 1955, and organized the first Artificial Intelligence conference in 1956, while working as a math teacher at Dartmouth. He founded the AI labs at MIT and Stanford.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;He's responsible for developing several other important concepts in today's mainstream computer science. Namely, he developed &lt;a href=&quot;https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)&quot; rel=&quot;noreferrer&quot;&gt;garbage collection&lt;/a&gt; (used by a Lisp interpreter) and designed the first &lt;a href=&quot;https://en.wikipedia.org/wiki/Time-sharing&quot; rel=&quot;noreferrer&quot;&gt;time-sharing systems&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side note, McCarthy predicted that creating a truly intelligent machine would require &lt;a href=&quot;https://www.nytimes.com/2011/10/26/science/26mccarthy.html&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;1.8 Einsteins and one-tenth the resources of the Manhattan Project&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="71" LastEditorUserId="2444" LastEditDate="2019-10-05T02:11:56.117" LastActivityDate="2019-10-05T02:11:56.117" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="67" PostTypeId="1" AcceptedAnswerId="72" CreationDate="2016-08-02T16:31:51.380" Score="6" ViewCount="5626" Body="&lt;p&gt;Why somebody would use SAT solvers (&lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&quot; rel=&quot;nofollow&quot;&gt;Boolean satisfiability problem&lt;/a&gt;) to solve their real world problems?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any examples of the real uses of this model?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="10135" LastEditDate="2018-10-18T10:45:09.917" LastActivityDate="2020-08-23T12:05:32.803" Title="What are the real world uses for SAT solvers?" Tags="&lt;models&gt;&lt;problem-solving&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="68" PostTypeId="1" CreationDate="2016-08-02T16:33:52.707" Score="6" ViewCount="191" Body="&lt;p&gt;What evolutionary algorithms are there that model or incorporate some notion of &lt;a href=&quot;https://en.wikipedia.org/wiki/Epigenetics&quot; rel=&quot;nofollow noreferrer&quot;&gt;epigenetics&lt;/a&gt;? What are the pros/cons of those approaches? Are there vast insufficiencies or wide-open questions about their usefulness?&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="2444" LastEditDate="2021-01-10T22:24:44.517" LastActivityDate="2021-01-10T22:24:44.517" Title="What evolutionary algorithms are there that model epigenetics?" Tags="&lt;reference-request&gt;&lt;genetic-algorithms&gt;&lt;evolutionary-algorithms&gt;&lt;genetic-programming&gt;&lt;evolutionary-computation&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="69" PostTypeId="2" ParentId="54" CreationDate="2016-08-02T16:34:26.163" Score="6" Body="&lt;p&gt;It doesn't make much sense to have a single threshold with &quot;unintelligent&quot; below it and &quot;intelligent&quot; above it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it makes more sense to have a gradation of intelligence by cognitive task. Inverting a matrix is a 'cognitive task,' and one where working memory pays off immensely; computers have been much better at that cognitive task than humans for a long time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What the AlphaGo victory represents has several components. One is that we have algorithms that are competitive with the best board-game playing humans at doing tactical and strategic thinking in the well-described world of Go. Another is that the deeper structure of the human visual system seems to have been duplicated, and so we have algorithms that can recognize patterns as well as humans--with &lt;em&gt;very&lt;/em&gt; limited resolution. (AlphaGo is seeing one pixel per stone, whereas we have very, very high-resolution eyes and the visual cortex to match.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Different people have different intuitions, but it seems to me that visual intelligence is a huge component of human intelligence in general. If we know most of the secrets of human visual intelligence, that means there might be many tasks that computers could now perform as well as humans (if provided the correct training data).&lt;/p&gt;&#xA;" OwnerUserId="10" LastActivityDate="2016-08-02T16:34:26.163" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="70" PostTypeId="1" CreationDate="2016-08-02T16:38:55.800" Score="26" ViewCount="436" Body="&lt;p&gt;Can a Convolutional Neural Network be used for pattern recognition in problem domains without image data? For example, by representing abstract data in an image-like format with spatial relations? Would that always be less efficient?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://youtu.be/py5byOOHZM8?t=815&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;This Developer&quot;&gt;This developer&lt;/a&gt; says current development could go further but not if there's a limit outside image recognition.&lt;/p&gt;&#xA;" OwnerUserId="46" LastEditorUserId="1641" LastEditDate="2021-01-11T19:03:23.563" LastActivityDate="2021-01-11T19:03:23.563" Title="Is the pattern recognition capability of CNNs limited to image processing?" Tags="&lt;deep-neural-networks&gt;&lt;neural-networks&gt;&lt;image-recognition&gt;&lt;convolutional-neural-networks&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="5" ContentLicense="CC BY-SA 4.0" />
  <row Id="71" PostTypeId="2" ParentId="46" CreationDate="2016-08-02T16:39:00.200" Score="11" Body="&lt;p&gt;The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 30s, 40s and early 50s (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Logic&quot; rel=&quot;nofollow noreferrer&quot;&gt;formal logic&lt;/a&gt;, automata, &lt;a href=&quot;https://en.wikipedia.org/wiki/Robot&quot; rel=&quot;nofollow noreferrer&quot;&gt;robots&lt;/a&gt;). Although the &lt;a href=&quot;https://academic.oup.com/mind/article/LIX/236/433/986238&quot; rel=&quot;nofollow noreferrer&quot;&gt;Turing test&lt;/a&gt; was proposed in the 1950s by &lt;a href=&quot;https://en.wikipedia.org/wiki/Alan_Turing&quot; rel=&quot;nofollow noreferrer&quot;&gt;Alan Turing&lt;/a&gt;, the work culminated back in the 1940s in the invention of the programmable digital computers, an abstract essence of mathematical reasoning. These ideas were inspired by a handful of scientists from a variety of fields who began seriously considering the possibility of building an electronic brain. &lt;a href=&quot;https://ojs.aaai.org//index.php/aimagazine/article/view/1904&quot; rel=&quot;nofollow noreferrer&quot;&gt;The field of artificial intelligence research was officially founded as an academic discipline in 1956 during the Dartmouth workshop&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However, the concept of artificial beings is not new and it's as old as the Greek myths of Hephaestus and Pygmalion, which incorporated the idea of intelligent robots (such as &lt;em&gt;Talos&lt;/em&gt;) and artificial beings (such as &lt;em&gt;Galatea&lt;/em&gt; and &lt;em&gt;Pandora&lt;/em&gt;).&lt;/p&gt;&#xA;&lt;p&gt;See the following articles at Wikipedia for further details:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_intelligence#History&quot; rel=&quot;nofollow noreferrer&quot;&gt;Artificial intelligence (AI)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_artificial_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;History of artificial intelligence&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence&quot; rel=&quot;nofollow noreferrer&quot;&gt;Timeline of artificial intelligence&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-01-31T16:46:15.343" LastActivityDate="2021-01-31T16:46:15.343" CommentCount="0" CommunityOwnedDate="2021-01-31T16:43:08.193" ContentLicense="CC BY-SA 4.0" />
  <row Id="72" PostTypeId="2" ParentId="67" CreationDate="2016-08-02T16:39:12.377" Score="4" Body="&lt;p&gt;Instead of talking about just SAT solvers, let me talk about optimization in general. Many economic problems can be cast as optimization problems: for example, FedEx may have a list of packages and the destinations for those packages, and must decide which packages to put on which trucks, and what order to deliver those packages in.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you write out a mathematical description of this problem, there are a truly stunning number of possible solutions, and a well-defined way to evaluate which of two solutions is better. A solver is an algorithm that will evaluate a solution, come up with another solution, and then evaluate that one, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In small cases and simple problems, the solver can also terminate with a proof that it is actually the best solution possible. But typically instead the solver just reports &quot;this is the best solution that I've seen,&quot; and that's used. An improvement in the solver means you can reliably get lower-cost solutions than you were seeing before.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the SAT problem specifically, the Wikipedia page on &lt;a href=&quot;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&quot; rel=&quot;nofollow&quot;&gt;SAT&lt;/a&gt; gives some examples:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Since the SAT problem is NP-complete, only algorithms with exponential worst-case complexity are known for it. In spite of this, efficient and scalable algorithms for SAT were developed over the last decade[when?] and have contributed to dramatic advances in our ability to automatically solve problem instances involving tens of thousands of variables and millions of constraints (i.e. clauses).[1] Examples of such problems in electronic design automation (EDA) include formal equivalence checking, model checking, formal verification of pipelined microprocessors,[12] automatic test pattern generation, routing of FPGAs,[14] planning, and scheduling problems, and so on. A SAT-solving engine is now considered to be an essential component in the EDA toolbox.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="10" LastEditorUserId="42" LastEditDate="2016-08-11T14:54:21.720" LastActivityDate="2016-08-11T14:54:21.720" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="73" PostTypeId="2" ParentId="37" CreationDate="2016-08-02T16:40:20.240" Score="10" Body="&lt;p&gt;A Markov model includes the probability of transitioning to each state considering the current state. &quot;Each state&quot; may be just one point - whether it rained on specific day, for instance - or it might look like multiple things - like a pair of words. You've probably seen automatically generated weird text that &lt;em&gt;almost&lt;/em&gt; makes sense, like &lt;a href=&quot;https://blog.codinghorror.com/markov-and-you/&quot;&gt;Garkov&lt;/a&gt; (the output of a Markov model based on the Garfield comic strips). That Coding Horror article also mentions the applications of Markov techniques to Google's PageRank.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Markov models are really only powerful when they have a lot of input to work with. If a machine looked through a lot of English text, it would get a pretty good idea of what words generally come after other words. Or after looking through someone's location history, it could figure out where that person is likely to go next from a certain place. Constantly updating the &quot;input corpus&quot; as more data is received would let the machine tune the probabilities of all the state transitions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Genetic algorithms are fairly different things. They create functions by shuffling around parts of functions and seeing how good each function is at a certain task. A child algorithm will depend on its parents, but Markov models are interested mostly in predicting what thing will come next in a sequence, not creating a new chunk of code. You might be able to use a Markov model to spit out a candidate function, though, depending on how simple the &quot;alphabet&quot; is. You could even then give more weight to the transitions in successful algorithms.&lt;/p&gt;&#xA;" OwnerUserId="75" LastActivityDate="2016-08-02T16:40:20.240" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="74" PostTypeId="1" AcceptedAnswerId="141" CreationDate="2016-08-02T16:42:35.817" Score="44" ViewCount="23335" Body="&lt;p&gt;I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2019-06-20T20:32:04.477" LastActivityDate="2019-06-20T20:32:04.477" Title="What is the difference between strong-AI and weak-AI?" Tags="&lt;terminology&gt;&lt;definitions&gt;&lt;agi&gt;&lt;comparison&gt;&lt;narrow-ai&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="12" ContentLicense="CC BY-SA 3.0" />
  <row Id="75" PostTypeId="1" CreationDate="2016-08-02T16:42:53.110" Score="0" ViewCount="114" Body="&lt;p&gt;As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The &quot;driver&quot; (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="33" LastEditDate="2016-08-02T18:00:37.377" LastActivityDate="2018-07-17T00:07:33.083" Title="What's the state of the art w.r.t research on the legal aspects of Artificial Intelligence?" Tags="&lt;legal&gt;" AnswerCount="1" CommentCount="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="76" PostTypeId="2" ParentId="54" CreationDate="2016-08-02T16:43:14.880" Score="9" Body="&lt;p&gt;There are at least two questions in your question: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;What are some of the methods used to program the successful go playing program?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Are those methods considered to be artificial intelligence?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The first question is deep and technical, the second broad and philosophical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The methods have been described in: &lt;a href=&quot;https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mastering the Game of Go with Deep Neural Networks and Tree Search&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem of Go or perfect information games in general is that:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;exhaustive search is infeasible.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So the methods will concentrate on shrinking the search space in an efficient way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Methods and structures described in the paper include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;learning from expert human players in a supervised fashion&lt;/li&gt;&#xA;&lt;li&gt;learning by playing against itself (reinforcement learning)&lt;/li&gt;&#xA;&lt;li&gt;Monte-Carlo tree search (MCTS) combined with policy and value networks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The second question has no definite answer, as you will have at least two angles on AI: &lt;a href=&quot;https://en.wikipedia.org/wiki/Chinese_room#Strong_AI&quot; rel=&quot;nofollow noreferrer&quot;&gt;strong&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_AI&quot; rel=&quot;nofollow noreferrer&quot;&gt;weak&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;All real-world systems labeled &quot;artificial intelligence&quot; of any sort are &lt;strong&gt;weak AI at most&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, yes, it is artificial intelligence, but it is non-sentient.&lt;/p&gt;&#xA;" OwnerUserId="28" LastEditorUserId="2444" LastEditDate="2020-03-09T21:29:34.550" LastActivityDate="2020-03-09T21:29:34.550" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="77" PostTypeId="1" AcceptedAnswerId="131" CreationDate="2016-08-02T16:45:01.487" Score="27" ViewCount="2467" Body="&lt;p&gt;I know that language of Lisp was used early on when working on artificial intelligence problems. Is it still being used today for significant work? If not, is there a new language that has taken its place as the most common one being used for work in AI today?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="14723" LastEditDate="2018-04-14T01:47:35.577" LastActivityDate="2021-01-10T23:03:14.497" Title="Is Lisp still being used to tackle AI problems?" Tags="&lt;history&gt;&lt;programming-languages&gt;&lt;lisp&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="7" ContentLicense="CC BY-SA 3.0" />
  <row Id="79" PostTypeId="2" ParentId="75" CreationDate="2016-08-02T16:45:51.657" Score="3" Body="&lt;p&gt;One person working in this space is Dr. Woody Barfield. He just wrote a book titled &lt;em&gt;&lt;a href=&quot;http://www.springer.com/us/book/9783319250489&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cyberhumans: Our Future With Machines&lt;/a&gt;&lt;/em&gt; that focuses largely on the legal/policy issues around AI (and related topics). In addition to the book, he is continuing with other research in this area.&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-14T02:00:29.083" LastActivityDate="2018-04-14T02:00:29.083" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="80" PostTypeId="1" AcceptedAnswerId="85" CreationDate="2016-08-02T16:46:07.253" Score="14" ViewCount="1866" Body="&lt;p&gt;What are the specific requirements of the Turing test? &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What requirements if any must the evaluator fulfill in order to be qualified to give the test?&lt;/li&gt;&#xA;&lt;li&gt;Must there always be two participants in the conversation (one human and one computer) or can there be more?&lt;/li&gt;&#xA;&lt;li&gt;Are placebo tests (where there is not actually a computer involved) allowed or encouraged?&lt;/li&gt;&#xA;&lt;li&gt;Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="96" LastEditorUserId="2444" LastEditDate="2020-05-18T23:55:50.073" LastActivityDate="2020-05-18T23:55:50.073" Title="What are the specific requirements of the Turing test?" Tags="&lt;natural-language-processing&gt;&lt;intelligence-testing&gt;&lt;turing-test&gt;" AnswerCount="3" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="81" PostTypeId="1" AcceptedAnswerId="87" CreationDate="2016-08-02T16:49:40.830" Score="8" ViewCount="2187" Body="&lt;p&gt;I believe that statistical AI uses inductive thought processes. For example, deducing a trend from a pattern, after training.&lt;/p&gt;&#xA;&lt;p&gt;What are some examples of successfully applied Statistical AI to real-world problems?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="2444" LastEditDate="2021-12-06T08:29:54.127" LastActivityDate="2021-12-06T08:29:54.127" Title="What are some examples of Statistical AI applications?" Tags="&lt;applications&gt;&lt;statistical-ai&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="82" PostTypeId="1" CreationDate="2016-08-02T16:50:15.330" Score="1" ViewCount="112" Body="&lt;p&gt;How do the basic components &lt;a href=&quot;https://en.wikipedia.org/wiki/Optimality_theory&quot; rel=&quot;nofollow&quot;&gt;optimality theory&lt;/a&gt; apply to artificial intelligence?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How is optimality theory related to neural network research?&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="2444" LastEditDate="2019-04-05T12:19:34.277" LastActivityDate="2019-04-05T12:19:34.277" Title="What is the relation between optimality theory and AI?" Tags="&lt;neural-networks&gt;&lt;applications&gt;&lt;comparison&gt;" AnswerCount="0" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="83" PostTypeId="2" ParentId="1" CreationDate="2016-08-02T16:54:40.380" Score="3" Body="&lt;p&gt;Yes, as Franck has rightly put, &quot;backprop&quot; means backpropogation, which is frequently used in the domain of neural networks for error optimization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a detailed explanation, I would point out &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap2.html&quot; rel=&quot;nofollow&quot;&gt;this tutorial&lt;/a&gt; on the concept of backpropogation by a very good book of Michael Nielsen. &lt;/p&gt;&#xA;" OwnerUserId="101" LastActivityDate="2016-08-02T16:54:40.380" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="84" PostTypeId="1" CreationDate="2016-08-02T16:55:37.050" Score="11" ViewCount="1041" Body="&lt;p&gt;Some programs do exhaustive searches for a solution while others do heuristic searches for a similar answer. For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas, in Go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI? If so, is the chess-playing computer beating a human professional seen as a meaningful milestone?&lt;/p&gt;&#xA;" OwnerUserId="55" LastEditorUserId="14723" LastEditDate="2018-04-14T02:03:13.207" LastActivityDate="2018-12-11T09:53:09.487" Title="Are methods of exhaustive search considered to be AI?" Tags="&lt;gaming&gt;&lt;search&gt;&lt;chess&gt;&lt;heuristics&gt;" AnswerCount="5" CommentCount="0" FavoriteCount="2" ContentLicense="CC BY-SA 3.0" />
  <row Id="85" PostTypeId="2" ParentId="80" CreationDate="2016-08-02T16:59:11.467" Score="10" Body="&lt;p&gt;The &quot;Turing Test&quot; is generally taken to mean an updated version of the Imitation Game Alan Turing proposed in his 1951 paper of the same name. An early version had a human (male or female) and a computer, and a judge had to decide which is which, and what gender they were if human. If they were correct less than 50% then the computer was considered &quot;intelligent.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The current generally accepted version requires only one contestant, and a judge to decide whether it is human or machine. So yes, sometimes this will be a placebo, effectively, if we consider a human to be a placebo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your first and fourth questions are related - and there are no strict guidelines. If the computer can fool a greater number of judges then it will of course be considered a better AI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The University of Toronto has a validity section in &lt;a href=&quot;http://www.psych.utoronto.ca/users/reingold/courses/ai/turing.html&quot;&gt;this paper on Turing&lt;/a&gt;, which includes a link to &lt;a href=&quot;http://ciips.ee.uwa.edu.au/Papers/Technical_Reports/1997/05/Index.html&quot;&gt;Jason Hutchens' commentary&lt;/a&gt; on why the Turing test may not be relevant (humans may also fail it) and the &lt;a href=&quot;http://www.loebner.net/Prizef/loebner-prize.html&quot;&gt;Loebner Prize&lt;/a&gt;, a formal instantiation of a Turing Test .&lt;/p&gt;&#xA;" OwnerUserId="97" LastEditorUserId="97" LastEditDate="2016-08-02T17:07:15.277" LastActivityDate="2016-08-02T17:07:15.277" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="86" PostTypeId="1" AcceptedAnswerId="93" CreationDate="2016-08-02T16:59:30.683" Score="31" ViewCount="1145" Body="&lt;p&gt;How is a neural network having the &quot;deep&quot; adjective actually distinguished from other similar networks?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-02-23T22:05:20.793" LastActivityDate="2020-08-23T11:06:25.767" Title="How is a deep neural network different from other neural networks?" Tags="&lt;neural-networks&gt;&lt;machine-learning&gt;&lt;deep-neural-networks&gt;&lt;terminology&gt;&lt;comparison&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="10" ContentLicense="CC BY-SA 4.0" />
  <row Id="87" PostTypeId="2" ParentId="81" CreationDate="2016-08-02T16:59:50.293" Score="10" Body="&lt;p&gt;There are several examples. For example, one instance of using Statistical AI from my workplace is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Analyzing the behavior of the customer and their food-ordering trends, and then trying to upsell by recommending them the dishes which they might like to order/eat. This can be done through the apriori and FP-growth algorithms. We then, automated the algorithm, and then the algorithm improves itself through an &lt;code&gt;Ordered/Not-Ordered&lt;/code&gt; metric.&lt;/li&gt;&#xA;&lt;li&gt;Self-driving cars. They use reinforcement and supervised learning algorithms for learning the route and the gradient/texture of the road.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="101" LastEditorUserId="14723" LastEditDate="2018-04-12T14:09:57.913" LastActivityDate="2018-04-12T14:09:57.913" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="88" PostTypeId="1" AcceptedAnswerId="2573" CreationDate="2016-08-02T17:01:18.317" Score="4" ViewCount="569" Body="&lt;p&gt;What is the effectiveness of pre-training of unsupervised deep learning?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does unsupervised deep learning actually work?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2019-11-19T21:16:17.260" LastActivityDate="2020-04-21T11:14:24.307" Title="Why does unsupervised pre-training help in deep learning?" Tags="&lt;deep-learning&gt;&lt;unsupervised-learning&gt;&lt;transfer-learning&gt;" AnswerCount="2" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="89" PostTypeId="2" ParentId="84" CreationDate="2016-08-02T17:02:05.890" Score="7" Body="&lt;p&gt;If a computer is just brute-forcing the solution, it's not learning anything or using any kind of intelligence at all, and therefore it shouldn't be called &quot;artificial intelligence.&quot; It has to make decisions based on what's happened before in similar instances. For something to be intelligent, it needs a way to keep track of what it's learned. A chess program might have a really awesome measurement algorithm to use on every possible board state, but if it's always trying each state and never storing what it learns about different approaches, it's not intelligent.&lt;/p&gt;&#xA;" OwnerUserId="75" LastEditorUserId="75" LastEditDate="2016-08-02T17:33:59.340" LastActivityDate="2016-08-02T17:33:59.340" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="91" PostTypeId="1" AcceptedAnswerId="97" CreationDate="2016-08-02T17:04:35.297" Score="17" ViewCount="431" Body="&lt;p&gt;Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this considered AI or just smart?&lt;/p&gt;&#xA;" OwnerUserId="5" LastEditorUserId="2444" LastEditDate="2019-06-24T13:10:54.557" LastActivityDate="2019-06-24T13:10:54.557" Title="Are search engines considered AI?" Tags="&lt;machine-learning&gt;&lt;philosophy&gt;&lt;definitions&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" ContentLicense="CC BY-SA 4.0" />
  <row Id="92" PostTypeId="1" AcceptedAnswerId="250" CreationDate="2016-08-02T17:05:27.590" Score="83" ViewCount="6721" Body="&lt;p&gt;The following &lt;a href=&quot;http://www.evolvingai.org/fooling&quot; rel=&quot;noreferrer&quot;&gt;page&lt;/a&gt;/&lt;a href=&quot;http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf&quot; rel=&quot;noreferrer&quot;&gt;study&lt;/a&gt; demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7pgrH.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7pgrH.jpg&quot; alt=&quot;Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &amp;gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/pBm48.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/pBm48.png&quot; alt=&quot;Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How this is possible? Can you please explain ideally in plain English?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-02-18T19:48:25.217" LastActivityDate="2021-02-18T19:48:25.217" Title="How is it possible that deep neural networks are so easily fooled?" Tags="&lt;convolutional-neural-networks&gt;&lt;computer-vision&gt;&lt;image-recognition&gt;&lt;deep-neural-networks&gt;&lt;adversarial-ml&gt;" AnswerCount="9" CommentCount="2" FavoriteCount="34" ContentLicense="CC BY-SA 3.0" />
  <row Id="93" PostTypeId="2" ParentId="86" CreationDate="2016-08-02T17:06:21.223" Score="32" Body="&lt;p&gt;The difference is mostly in the number of layers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a long time, it was believed that &quot;1-2 hidden layers are enough for most tasks&quot; and it was impractical to use more than that, because training neural networks can be very computationally demanding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, computers are capable of much more, so people have started to use networks with more layers and found that they work very well for some tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The word &quot;deep&quot; is there simply to distinguish these networks from the traditional, &quot;more shallow&quot; ones.&lt;/p&gt;&#xA;" OwnerUserId="30" LastEditorUserId="2444" LastEditDate="2019-02-23T22:08:58.833" LastActivityDate="2019-02-23T22:08:58.833" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="94" PostTypeId="1" AcceptedAnswerId="132" CreationDate="2016-08-02T17:06:46.317" Score="3" ViewCount="314" Body="&lt;p&gt;In a feedforward neural network, the inputs are fed directly to the outputs via a series of &lt;strong&gt;weights&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;What purpose do the weights serve, and how are they significant in this neural network?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2444" LastEditDate="2021-12-04T09:23:57.703" LastActivityDate="2021-12-04T09:23:57.703" Title="What is the significance of weights in a feedforward neural network?" Tags="&lt;neural-networks&gt;&lt;weights&gt;&lt;perceptron&gt;&lt;feedforward-neural-networks&gt;" AnswerCount="1" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="95" PostTypeId="2" ParentId="86" CreationDate="2016-08-02T17:08:48.340" Score="10" Body="&lt;p&gt;A deep neural network is just a (feed-forward) neural network with many layers.&lt;/p&gt;&#xA;&lt;p&gt;However, deep belief networks, Deep Boltzmann networks, etc., are not considered (debatable) deep neural networks, as their topology is different (i.e. they have undirected networks in their topology).&lt;/p&gt;&#xA;&lt;p&gt;See also this: &lt;a href=&quot;https://stats.stackexchange.com/a/59854/84191&quot;&gt;https://stats.stackexchange.com/a/59854/84191&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2020-08-23T11:06:25.767" LastActivityDate="2020-08-23T11:06:25.767" CommentCount="0" ContentLicense="CC BY-SA 4.0" />
  <row Id="96" PostTypeId="1" AcceptedAnswerId="98" CreationDate="2016-08-02T17:12:58.533" Score="11" ViewCount="318" Body="&lt;p&gt;What is the definition of a deep neural network? Why are they so popular or important?&lt;/p&gt;&#xA;" OwnerUserId="5" LastEditorUserId="2444" LastEditDate="2019-06-23T12:18:37.573" LastActivityDate="2020-04-21T11:25:25.153" Title="What is a deep neural network?" Tags="&lt;machine-learning&gt;&lt;deep-learning&gt;&lt;terminology&gt;&lt;deep-neural-networks&gt;&lt;definitions&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="0" ClosedDate="2020-08-23T11:03:13.587" ContentLicense="CC BY-SA 4.0" />
  <row Id="97" PostTypeId="2" ParentId="91" CreationDate="2016-08-02T17:15:22.887" Score="22" Body="&lt;p&gt;I believe it would be more accurate to say that (some) search engines &lt;em&gt;use&lt;/em&gt; AI.  Broadly saying &quot;search engines are AI&quot; is not really correct. At the core, most search engines are nothing more than an inverted text index using something like tf–idf scoring. That's a very mechanical/simple thing that nobody would really call AI. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But more sophisticated search engines may &lt;em&gt;use&lt;/em&gt; AI or AI techniques to do things like semantic analysis - so they can actually &quot;answer questions&quot; instead of just looking up words in an index.  &lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="14723" LastEditDate="2018-04-13T18:07:34.777" LastActivityDate="2018-04-13T18:07:34.777" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
  <row Id="98" PostTypeId="2" ParentId="96" CreationDate="2016-08-02T17:18:12.383" Score="12" Body="&lt;p&gt;A deep neural network (DNN) is nothing but a neural network which has multiple layers, where &lt;em&gt;multiple&lt;/em&gt; can be subjective.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;IMHO, any network which has 6 or 7 or more layers is considered deep. So, the above would form a very basic definition of a deep network. &lt;/p&gt;&#xA;" OwnerUserId="101" LastEditorUserId="2444" LastEditDate="2019-06-24T10:56:42.927" LastActivityDate="2019-06-24T10:56:42.927" CommentCount="1" ContentLicense="CC BY-SA 4.0" />
  <row Id="100" PostTypeId="2" ParentId="96" CreationDate="2016-08-02T17:36:52.900" Score="5" Body="&lt;p&gt;Deep networks have two main differences with 'normal' networks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first is that computational power and training datasets have grown immensely, meaning that it's practical to run larger networks and statistically valid (that is, we have enough training examples that we won't just run into over-fitting problems with larger networks).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second is that back propagation is limited the more layers you have; each layer represents a gradient of the error, and so by the time one is about six layers deep there isn't much error left to modify the neuron weights. But one might reasonably expect earlier neurons to be more important than later neurons, since they represent 'concepts' that are closer to the raw inputs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;New training techniques sidestep this problem, typically by doing unsupervised learning on the raw inputs, creating higher-level 'concepts' that are then useful as inputs for supervised learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(For example, consider the problem of determining whether or not an image contains a cat from the pixels. The early layers of the network should be doing things like detecting edges, which one could expect to be shared among all images and mostly independent of what one is trying to do with the output layers, thus also hard to train through 'cat-not cat' signals many layers up.&lt;/p&gt;&#xA;" OwnerUserId="10" LastEditorUserId="5" LastEditDate="2016-08-03T15:13:11.603" LastActivityDate="2016-08-03T15:13:11.603" CommentCount="0" ContentLicense="CC BY-SA 3.0" />
</posts>